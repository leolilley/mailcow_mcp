# Testing Guide for Mailcow MCP Server

This guide explains the testing infrastructure and how to use code coverage effectively.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ unit/                        # Unit tests (fast, isolated)
â”‚   â”œâ”€â”€ auth/                    # Auth system tests (4 files, 70% coverage)
â”‚   â””â”€â”€ utils/                   # Utility tests (8 files, 77% coverage)
â””â”€â”€ integration/                 # Integration tests (slower, end-to-end)
    â””â”€â”€ server-startup.test.js   # Full server startup test
```

## ğŸ§ª Running Tests

### Basic Commands
```bash
# Run all unit tests
npm test
# or specifically
npm run test:unit

# Run integration tests
npm run test:integration

# Run both unit and integration tests
npm run test:all

# Generate coverage report
npm run test:coverage

# Generate coverage report AND open in browser ğŸŒ
npm run test:coverage:open

# Just open existing coverage report
npm run coverage:open
```

### Test Types

**Unit Tests** (Fast, ~10s):
- âœ… **Auth module**: 70% coverage, 162 test cases
- âœ… **Utils module**: 77% coverage, comprehensive validation
- âŒ **API module**: 0% coverage (needs implementation)
- âŒ **Tools module**: 0% coverage (needs implementation)
- âŒ **Config module**: 0% coverage (needs implementation)

**Integration Tests** (Slower, ~3s):
- âœ… **Server startup**: Tests full MCP server initialization and protocol response
- âŒ **Tool execution**: Not implemented yet
- âŒ **API integration**: Not implemented yet

## ğŸ“Š Code Coverage System

### What is the `coverage/` folder?

The `coverage/` folder contains detailed reports about which parts of your code are tested:

```
coverage/
â”œâ”€â”€ index.html              # ğŸŒ Main coverage dashboard (open in browser)
â”œâ”€â”€ lcov.info              # ğŸ“„ Machine-readable coverage data  
â”œâ”€â”€ lcov-report/            # ğŸŒ Detailed HTML reports by file
â”‚   â”œâ”€â”€ auth/               # Auth module coverage breakdown
â”‚   â”œâ”€â”€ utils/              # Utils module coverage breakdown
â”‚   â”œâ”€â”€ api/                # API module coverage (0% - needs work!)
â”‚   â””â”€â”€ tools/              # Tools module coverage (0% - needs work!)
â””â”€â”€ [other files]          # Supporting files (CSS, JS, etc.)
```

### ğŸ“ˆ How to Use Coverage Reports

#### 1. **View Overall Coverage** 
```bash
npm run test:coverage
```
**Current Status**: 18.55% overall
- **ğŸŸ¢ Auth**: 70.16% (excellent!)
- **ğŸŸ¢ Utils**: 77.16% (excellent!)  
- **ğŸ”´ API**: 0% (critical gap)
- **ğŸ”´ Tools**: 0% (critical gap)
- **ğŸ”´ Config**: 0% (needs basic tests)

#### 2. **Interactive HTML Reports**
**Quick Access**: `npm run coverage:open` ğŸš€

Open `coverage/index.html` in your browser for:
- **ğŸ“Š Visual coverage dashboard** with sortable tables
- **ğŸ¯ File-by-file breakdown** - click any file to see detailed coverage
- **ğŸ“ Line-by-line analysis** - see exactly which lines are tested (green) vs untested (red)
- **ğŸ” Branch coverage** - see which code paths are exercised

#### 3. **Understanding Coverage Metrics**

**Statements**: % of executable code lines run during tests
**Branches**: % of if/else, switch, ternary conditions tested  
**Functions**: % of functions called during tests
**Lines**: % of code lines executed (excluding comments/whitespace)

### ğŸ¯ Coverage Goals

**Target Coverage Levels**:
- **Critical modules** (auth, API client): >80%  
- **Utility modules**: >70%
- **Tool implementations**: >60%
- **Overall project**: >50%

**Current Priority**:
1. **ğŸš¨ API Client tests** (currently 0%) - critical for reliability
2. **ğŸš¨ Tool registry tests** (currently 0%) - core functionality  
3. **âš ï¸ Config validation tests** (currently 0%) - important for stability

### ğŸ” Finding What to Test

**Use coverage reports to identify**:
1. **Red lines** in HTML reports = untested code paths
2. **Low function coverage** = functions never called in tests
3. **Low branch coverage** = missing error cases or edge conditions

**Example workflow**:
```bash
# 1. Run coverage
npm run test:coverage

# 2. Open coverage/index.html in browser

# 3. Navigate to src/api/client.ts.html  

# 4. Look for red highlighted lines

# 5. Write tests to cover those code paths

# 6. Re-run coverage to verify improvement
```

## ğŸ“ Writing New Tests

### Unit Test Template
```typescript
// tests/unit/api/client.test.ts
import { APIClient } from '../../../src/api/client';

describe('APIClient', () => {
  let client: APIClient;
  
  beforeEach(() => {
    client = new APIClient({
      url: 'https://test.example.com',
      key: 'test-api-key-with-32-characters',
      accessType: 'read-only'
    });
  });

  it('should initialize with correct config', () => {
    expect(client).toBeDefined();
    // Add your assertions
  });

  it('should handle API errors gracefully', async () => {
    // Test error scenarios
  });
});
```

### Integration Test Template  
```javascript
// tests/integration/api-integration.test.js
const { spawn } = require('child_process');

// Set up test environment
process.env.MAILCOW_API_URL = 'https://test.example.com';
process.env.MAILCOW_API_KEY = 'test-key-32-characters-long';

// Test full end-to-end functionality
```

## ğŸ¯ Testing Best Practices

### 1. **Test the Critical Path First**
- API client HTTP operations
- Authentication validation  
- Tool execution workflows
- Error handling

### 2. **Mock External Dependencies**
```typescript
// Use existing MockLogger for consistent testing
import { MockLogger } from '../../../src/utils/mocks';

const logger = new MockLogger();
// ... run code that logs
expect(logger.getLogs()).toHaveLength(2);
```

### 3. **Test Both Success and Failure Cases**
```typescript
it('should handle valid API key', async () => {
  const result = await auth.validateAPIKey('a'.repeat(32));
  expect(result.success).toBe(true);
});

it('should reject invalid API key', async () => {
  const result = await auth.validateAPIKey('short');
  expect(result.success).toBe(false);
  expect(result.error?.code).toBe('INVALID_API_KEY');
});
```

### 4. **Use Coverage to Guide Testing**
- Aim for >80% coverage on critical modules
- Focus on untested branches (error cases)
- Don't chase 100% coverage - focus on important code paths

## ğŸ“ˆ Current Test Status Summary

| Module | Coverage | Priority | Status |
|--------|----------|----------|--------|
| **Auth** | 70% | High | âœ… Well tested |
| **Utils** | 77% | High | âœ… Well tested |  
| **API Client** | 0% | Critical | ğŸš¨ Needs immediate attention |
| **Tools** | 0% | Critical | ğŸš¨ Needs immediate attention |
| **Config** | 0% | Medium | âš ï¸ Needs basic tests |
| **Types** | 71% | Low | âœ… Adequate |

**Next Steps**:
1. Add API client tests with HTTP mocking
2. Add tool registry and execution tests  
3. Add configuration validation tests
4. Add end-to-end integration tests

The testing foundation is excellent - now we need to extend it to cover the core application logic!